#!/usr/bin/env python3
"""
train_doc2vec.py
================
End-to-end Doc2Vec training pipeline for any GitHub repository.

Features:
- Clones a GitHub repo
- Extracts source code files (configurable by extension)
- Tokenizes code
- Trains a Doc2Vec model (Gensim)
- Saves embeddings and model for downstream use

Usage:
  python train_doc2vec.py \
      --repo https://github.com/apache/calcite.git \
      --ext .java \
      --output calcite_embeddings.csv
"""

import os
import re
import sys
import subprocess
import tempfile
from pathlib import Path
from tqdm import tqdm
import pandas as pd
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
import argparse


# ----------------------------------------------------------
# 1Ô∏è‚É£ CLONE REPOSITORY
# ----------------------------------------------------------
def clone_repo(github_url: str, dest_dir: str = None) -> Path:
    """Clone a GitHub repository."""
    if dest_dir is None:
        dest_dir = tempfile.mkdtemp(prefix="repo_")
    subprocess.run(["git", "clone", "--depth", "1", github_url, dest_dir], check=True)
    print(f"‚úÖ Repository cloned to: {dest_dir}")
    return Path(dest_dir)


# ----------------------------------------------------------
# 2Ô∏è‚É£ EXTRACT SOURCE FILES
# ----------------------------------------------------------
def get_source_files(repo_path: Path, extensions: list[str]) -> list[Path]:
    """Collect source files matching given extensions."""
    files = []
    for ext in extensions:
        files.extend(repo_path.rglob(f"*{ext}"))
    print(f"üìÑ Found {len(files)} source files ({', '.join(extensions)})")
    return files


# ----------------------------------------------------------
# 3Ô∏è‚É£ TOKENIZE CODE
# ----------------------------------------------------------
def tokenize_code(code: str) -> list[str]:
    """Simple regex-based code tokenizer."""
    tokens = re.findall(r"[A-Za-z_][A-Za-z_0-9]*", code)
    return [t.lower() for t in tokens if len(t) > 1]


# ----------------------------------------------------------
# 4Ô∏è‚É£ PREPARE DOCUMENTS
# ----------------------------------------------------------
def prepare_documents(files: list[Path]) -> list[TaggedDocument]:
    """Prepare TaggedDocument objects for training."""
    documents = []
    for file_path in tqdm(files, desc="Tokenizing files"):
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                tokens = tokenize_code(f.read())
                if len(tokens) > 5:
                    documents.append(TaggedDocument(words=tokens, tags=[str(file_path)]))
        except Exception as e:
            print(f"‚ö†Ô∏è Skipping {file_path}: {e}")
    print(f"üìö Prepared {len(documents)} documents for training")
    return documents


# ----------------------------------------------------------
# 5Ô∏è‚É£ TRAIN DOC2VEC MODEL
# ----------------------------------------------------------
def train_doc2vec(
    documents: list[TaggedDocument],
    vector_size: int = 200,
    window: int = 5,
    min_count: int = 3,
    epochs: int = 20,
    dm: int = 1,
) -> Doc2Vec:
    """Train a Doc2Vec model using the given documents."""
    print("üöÄ Training Doc2Vec model ...")
    model = Doc2Vec(
        documents,
        vector_size=vector_size,
        window=window,
        min_count=min_count,
        epochs=epochs,
        dm=dm,
        workers=os.cpu_count() or 2,
    )
    print("‚úÖ Doc2Vec model training complete")
    return model


# ----------------------------------------------------------
# 6Ô∏è‚É£ EXPORT EMBEDDINGS
# ----------------------------------------------------------
def export_embeddings(model: Doc2Vec, documents: list[TaggedDocument], output_file: str) -> pd.DataFrame:
    """Infer and export embeddings to CSV."""
    print("üíæ Exporting embeddings ...")
    data = []
    for doc in tqdm(documents, desc="Inferring embeddings"):
        vec = model.infer_vector(doc.words)
        data.append([doc.tags[0]] + vec.tolist())

    df = pd.DataFrame(data, columns=["file_path"] + [f"dim_{i}" for i in range(len(data[0]) - 1)])
    df.to_csv(output_file, index=False)
    print(f"‚úÖ Embeddings saved to {output_file}")
    return df


# ----------------------------------------------------------
# 7Ô∏è‚É£ MAIN PIPELINE
# ----------------------------------------------------------
def run_pipeline(github_url: str, extensions: list[str], output_file: str):
    repo_dir = clone_repo(github_url)
    source_files = get_source_files(repo_dir, extensions)
    if not source_files:
        print("‚ùå No source files found. Check your extensions or repo path.")
        sys.exit(1)

    documents = prepare_documents(source_files)
    model = train_doc2vec(documents)
    export_embeddings(model, documents, output_file)

    model_path = Path(output_file).with_suffix(".model")
    model.save(str(model_path))
    print(f"‚úÖ Model saved as {model_path}")
    print("üéâ Pipeline finished successfully!")


# ----------------------------------------------------------
# 8Ô∏è‚É£ CLI ENTRYPOINT
# ----------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train Doc2Vec on a GitHub repository.")
    parser.add_argument("--repo", required=True, help="GitHub repository URL")
    parser.add_argument("--ext", nargs="+", default=[".java"], help="File extensions to include (e.g., .py .java)")
    parser.add_argument("--output", default="embeddings.csv", help="Output CSV file for embeddings")

    args = parser.parse_args()
    run_pipeline(args.repo, args.ext, args.output)
